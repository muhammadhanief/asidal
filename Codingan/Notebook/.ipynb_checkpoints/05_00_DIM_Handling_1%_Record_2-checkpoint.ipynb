{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35385d3e-59c9-4b58-ab63-a816dcc7781e",
   "metadata": {},
   "source": [
    "# GET INDONESIAN AIS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72994dbe-2516-4416-be4f-4b1f71c13ff4",
   "metadata": {},
   "source": [
    "## SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455300b0-c299-4d3d-9c23-11999f899f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sedona Imports\n",
    "import sedona.sql\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.core.SpatialRDD import PolygonRDD, PointRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.types as pst\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType,LongType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1760fb-c83e-4b46-97dc-5155779d8ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('Emissions_Indonesia'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages'). \\\n",
    "    config(\"spark.sql.parquet.enableVectorizedReader\", \"false\").\\\n",
    "    getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb32de-8347-42c4-a788-6d9e0b52b31c",
   "metadata": {},
   "source": [
    "## AIS DATA IN S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bea538-190e-4403-907f-81ee94a76bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"s3a://ungp-ais-data-historical-backup/user_temp/\"\n",
    "save_path_unique = save_path + \"222011485/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132336d8-2ad8-4e93-9d47-3b4ccfbaf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read saved parquet\n",
    "data = spark.read.parquet(save_path_unique + \"ais-data-indonesia-2022.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94fe5a-38b1-4dbb-ab8f-62a3fffe07bd",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d883c1eb-8c76-4ed1-9887-53621f10cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "###Packages within Kernel by default\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95668acf",
   "metadata": {},
   "source": [
    "## 1. Preproses Record AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72aa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def compare_vessel_name(name_1, name_2):\n",
    "    vector1 = text_to_vector(name_1)\n",
    "    vector2 = text_to_vector(name_2)\n",
    "\n",
    "    cosine_result = get_cosine(vector1, vector2)\n",
    "    return cosine_result\n",
    "\n",
    "compare = F.udf(lambda x,y:compare_vessel_name(x,y),T.DoubleType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb06fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data = data.distinct()\n",
    "\n",
    "specs = spark.read.load(\"s3a://ungp-ais-data-historical-backup/register/ShipData.CSV\",format=\"csv\",sep=\",\",inferSchema=\"true\",header=\"true\")\n",
    "specs = specs.withColumnRenamed(\"MaritimeMobileServiceIdentityMMSINumber\",\"mmsi_ihs\")\\\n",
    "                .withColumnRenamed(\"LRIMOShipNo\",\"imo_ihs\")\\\n",
    "                .withColumnRenamed(\"Draught\",\"SummerDraught\")\n",
    "\n",
    "imo_match = ais_data\\\n",
    "                    .join(specs, (ais_data.imo == specs.imo_ihs),how=\"inner\")\\\n",
    "                    .withColumn(\"matchBy\", F.lit(\"imo\"))\n",
    "\n",
    "ais_ihs_left = ais_data.join(specs, (ais_data.imo == specs.imo_ihs),how=\"left_anti\")\n",
    "mmsi_match = ais_ihs_left.join(specs, (ais_ihs_left.mmsi == specs.mmsi_ihs),how=\"inner\")\n",
    "\n",
    "vessel_name_match = mmsi_match.withColumn(\"similarity\", compare(F.col(\"vessel_name\"), F.col(\"ShipName\"))).filter(F.col(\"similarity\")>=0.50)\n",
    "vessel_name_match = vessel_name_match.withColumn(\"imo\", F.col(\"imo_ihs\"))\\\n",
    "                                        .withColumn(\"matchBy\", F.lit(\"mmsi\"))\n",
    "\n",
    "match_record = imo_match.union(vessel_name_match.drop(F.col(\"similarity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b89d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|Jumlah record|          Keterangan|\n",
      "+-------------+--------------------+\n",
      "|    219038333|Record AIS Indone...|\n",
      "|    218717372|Penghapusan duplikat|\n",
      "|    193917476|Pencocokan dengan...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_filter = spark.createDataFrame([\n",
    "    {\"Keterangan\": \"Record AIS Indonesia Tahun 2022\", \"Jumlah record\": data.count()},\n",
    "    {\"Keterangan\": \"Penghapusan duplikat\", \"Jumlah record\": ais_data.count()},\n",
    "    {\"Keterangan\": \"Pencocokan dengan database IHS\", \"Jumlah record\": match_record.count()}\n",
    "])\n",
    "table_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1a2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|Jumlah record|          Keterangan|\n",
      "+-------------+--------------------+\n",
      "|    181377200|    Cocok dengan IMO|\n",
      "|     15383616|   Cocok dengan MMSI|\n",
      "|     12540276|Cocok dengan Nama...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_match = spark.createDataFrame([\n",
    "    {\"Jumlah record\": imo_match.count(), \"Keterangan\": \"Cocok dengan IMO\"},\n",
    "    {\"Jumlah record\": mmsi_match.count(), \"Keterangan\": \"Cocok dengan MMSI\"},\n",
    "    {\"Jumlah record\": vessel_name_match.count(), \"Keterangan\": \"Cocok dengan Nama Kapal\"}\n",
    "])\n",
    "table_match.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9645d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|Jumlah record|          Keterangan|\n",
      "+-------------+--------------------+\n",
      "|        25057|    Cocok dengan IMO|\n",
      "|         4048|Cocok dengan MMSI...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_vessel = spark.createDataFrame([\n",
    "    {\"Jumlah record\": imo_match.dropDuplicates([\"imo\"]).count(), \"Keterangan\": \"Cocok dengan IMO\"},\n",
    "    {\"Jumlah record\": vessel_name_match.dropDuplicates([\"imo\"]).count(), \"Keterangan\": \"Cocok dengan MMSI dan Nama Kapal\"}\n",
    "])\n",
    "table_vessel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e0636",
   "metadata": {},
   "source": [
    "# 2. Preproses Database IHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dcda6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mendapatkan kapal unik\n",
    "match_vessel = match_record.dropDuplicates([\"imo\"])\n",
    "# match_vessel.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86b472",
   "metadata": {},
   "source": [
    "## 2.1 Pengecekan Nilai Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bea372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " GrossTonnage             | 0    \n",
      " Deadweight               | 0    \n",
      " LengthOverallLOA         | 0    \n",
      " DateOfBuild              | 0    \n",
      " TEU                      | 0    \n",
      " Powerkwmax               | 49   \n",
      " MainEngineModel          | 532  \n",
      " Speed                    | 0    \n",
      " Speedmax                 | 0    \n",
      " Speedservice             | 0    \n",
      " BreadthExtreme           | 0    \n",
      " SummerDraught            | 0    \n",
      " FuelType1Capacity        | 404  \n",
      " FuelType2Capacity        | 0    \n",
      " LightDisplacementTonnage | 0    \n",
      " MainEngineRPM            | 1130 \n",
      " MainEngineType           | 107  \n",
      " Powerkwservice           | 0    \n",
      " PropulsionType           | 0    \n",
      " ShiptypeLevel5           | 0    \n",
      " TotalBunkerCapacity      | 9884 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['GrossTonnage', 'Deadweight', 'LengthOverallLOA',\n",
    "    'DateOfBuild', 'TEU', 'Powerkwmax', 'MainEngineModel', 'Speed', 'Speedmax', 'Speedservice', 'BreadthExtreme', 'SummerDraught', 'FuelType1Capacity',\n",
    "    'FuelType2Capacity', 'LightDisplacementTonnage', 'MainEngineRPM', 'MainEngineType', 'Powerkwservice', 'PropulsionType',\n",
    "    'ShiptypeLevel5', 'TotalBunkerCapacity']\n",
    "\n",
    "match_vessel.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab1a116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " GrossTonnage             | 15    \n",
      " Deadweight               | 3043  \n",
      " LengthOverallLOA         | 250   \n",
      " TEU                      | 24837 \n",
      " Powerkwmax               | 422   \n",
      " Speed                    | 3662  \n",
      " Speedmax                 | 15002 \n",
      " Speedservice             | 4860  \n",
      " BreadthExtreme           | 11691 \n",
      " SummerDraught            | 1604  \n",
      " FuelType1Capacity        | 9596  \n",
      " FuelType2Capacity        | 11492 \n",
      " LightDisplacementTonnage | 9965  \n",
      " MainEngineRPM            | 0     \n",
      " Powerkwservice           | 19880 \n",
      " TotalBunkerCapacity      | 0     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['GrossTonnage', 'Deadweight', 'LengthOverallLOA',\n",
    "    'TEU', 'Powerkwmax', 'Speed', 'Speedmax', 'Speedservice', 'BreadthExtreme', 'SummerDraught', 'FuelType1Capacity',\n",
    "    'FuelType2Capacity', 'LightDisplacementTonnage', 'MainEngineRPM', 'Powerkwservice', 'TotalBunkerCapacity']\n",
    "\n",
    "match_vessel.select([F.count(F.when(F.col(c)==0, c)).alias(c) for c in columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c01e2",
   "metadata": {},
   "source": [
    "# 3. Penyesuaian Spesifikasi Kapal Sesuai IMO GHG 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1baa7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'XY'}\n",
    "\n",
    "map_vessel_type = \"https://raw.githubusercontent.com/nandyarz/ais/main/map_vessel_type_imo4.json\"\n",
    "map_vessel_type=pd.read_json(map_vessel_type)\n",
    "\n",
    "#Types Table\n",
    "vessel_type = \"https://raw.githubusercontent.com/nandyarz/ais/main/type_table.json\"\n",
    "vessel_type=pd.read_json(vessel_type)\n",
    "\n",
    "#####Functions--------------------\n",
    "##Cleaning punctuations from new ais_types\n",
    "def clean_string(text):\n",
    "    text=''.join([word for word in text if word not in string.punctuation])\n",
    "    text=text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "##base shiptypelevel5 to comapre to\n",
    "base_stype=map_vessel_type.ShiptypeLevel5.unique().tolist()\n",
    "\n",
    "##Compare similiraty higher than 50% and return respective shiptype5 value\n",
    "def compare_similarity(text):\n",
    "    comp=[text]+base_stype\n",
    "    cleaned=list(map(clean_string,comp))\n",
    "    vectors=CountVectorizer().fit_transform(cleaned)\n",
    "    vectors=vectors.toarray()\n",
    "    csim=cosine_similarity(vectors)\n",
    "\n",
    "    val_com=np.max(csim[0,1:])\n",
    "\n",
    "    if val_com>0.75:\n",
    "        v_type=base_stype[np.argmax(csim[0,1:])]\n",
    "    else:\n",
    "        v_type=None\n",
    "\n",
    "    return v_type   \n",
    "\n",
    "##Imo bin finder\n",
    "def bin_finder(vessel_t,value,df_in):\n",
    "    try:\n",
    "        bin_imo=df_in[((df_in.StandardVesselType==vessel_t)&(df_in.mindiff<=value)&(df_in.maxdiff>=value))].imo4bin.iloc[0]\n",
    "    except:\n",
    "        bin_imo=0\n",
    "    return bin_imo\n",
    "\n",
    "def ihs_preprocessing(df):\n",
    "    columns = ['Powerkwmax', 'Speed', 'Speedmax', 'SummerDraught', 'MainEngineRPM']\n",
    "    \n",
    "    for c in columns:\n",
    "        df[c].replace(0, np.nan, inplace=True)\n",
    "        df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
    "    return df\n",
    "\n",
    "###++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "###Unit of cargo measurement per vessel type\n",
    "unit={'Bulk carrier':'Deadweight',\n",
    "     'Chemical tanker':'Deadweight',\n",
    "     'Container':\"TEU\",\n",
    "     'General cargo':'Deadweight',\n",
    "     'Liquified gas tanker':'GrossTonnage',\n",
    "     'Oil tanker':'Deadweight',\n",
    "     'Other liquids tankers':'Deadweight',\n",
    "     'Ferry-pax only':'GrossTonnage',\n",
    "     'Cruise':'GrossTonnage',\n",
    "     'Ferry-RoPax':'GrossTonnage',\n",
    "     'Refrigerated bulk':'Deadweight',\n",
    "     'Ro-Ro':'Deadweight',\n",
    "     'Vehicle':'GrossTonnage',\n",
    "     'Yacht':'GrossTonnage',\n",
    "     'Service-tug':'GrossTonnage',\n",
    "     'Miscellaneous-fishing':'GrossTonnage',\n",
    "     'Offshore':'GrossTonnage',\n",
    "     'Service-other':'GrossTonnage',\n",
    "     'Miscellaneous-other':'GrossTonnage'}\n",
    "\n",
    "##Engine type allocation\n",
    "oil_eng=['Diesel-Elec & Gas Turbine(s)','Oil Engs & Fuel Cell-Electric''Oil Eng(s), Elec-Dr, Aux Sail','Oil Engines, Geared & Elec. Dr','Oil Eng(s) & Gas Turb(s) El.Dr','Oil Eng(s) Direct Dr, Aux Sail','Oil Eng(s) Dd & Gas Turb(s) El','Oil Engines, F&S, Geared Drive','Oil Engines, Direct & Elec. Dr','Oil Engines, Elec. & Direct Dr','Oil Engine(s), Drive Unknown','Oil Engines, Elec. & Geared Dr','Oil Eng(s), Geared, Aux Sail','Oil Engs & Gas Turb(s), Geared','Oil Engine(s), Electric Drive','Oil Engine(s), Direct Drive','Oil Engine(s), Geared Drive']\n",
    "sail=['Sail, Aux Petrol Eng(s) D.Dr.','Sail, Aux Oil Eng(s), Elec Dr.','Sail, Aux Oil Eng(s), Geared','Sail','Sail, Aux Oil Eng(s) Direct-Dr',]\n",
    "gas_tur=['Gas Turbine(s), Electric Drive','Gas Turbine(s) Or Diesel Elec.','Gas Turbine(s) & Diesel Elec.','Gas Turbine(s), Geared Drive',]\n",
    "steam=['S.Turb, Gear & Oil Eng(s), Ele','St. Turb(s) Elec Dr. Or D.E.','Steam Turbine(s), Direct Drive','Steam Recip(s) With Lp Turbine','Steam Turbine(s), Elec.Drive','Steam- & Gas-Turbines, Geared','Steam Turbine(s), Geared Drive','Steam Recip(s), Direct Drive',]\n",
    "\n",
    "\n",
    "def adapted_specs_imo(df_unique_imo):\n",
    "    df_unique_imo.rename(columns={\"vessel_type_main\":\"ais_type\",\"length\":\"ais_loa\",\"width\":\"ais_beam\"},inplace=True)\n",
    "\n",
    "    ind=df_unique_imo.copy()\n",
    "\n",
    "    ind=ind.assign(ShiptypeLevel5=np.where(ind.ShiptypeLevel5.isnull(),ind.ais_type,ind.ShiptypeLevel5))\n",
    "    ##Remove values with Shiptypelevel5 null. Not much else to do with this records. \n",
    "    ##Remove nans before similarity check\n",
    "    ind=ind[ind.ShiptypeLevel5.notnull()]\n",
    "    ind=ind.assign(ShiptypeLevel5=np.where(ind.ShiptypeLevel5.isin(base_stype),ind.ShiptypeLevel5,\n",
    "                                        ind.ShiptypeLevel5.apply(lambda x: compare_similarity(x))))\n",
    "    ##Ensure no vessel without Standard vessel type\n",
    "    ind=ind[ind.ShiptypeLevel5.notnull()]\n",
    "\n",
    "    ##---Pending----Inputation here input from AIS(Length,Beam) and Shiptypelevel5 to have [DWT,GT]. Potential RF Regressor (missForest).\n",
    "\n",
    "    ind=pd.merge(ind,map_vessel_type,how=\"left\",on='ShiptypeLevel5')\n",
    "\n",
    "    ind=ind.assign(imobin=ind.apply(lambda x: bin_finder(x.StandardVesselType,x[unit[x.StandardVesselType]],vessel_type),axis=1))\n",
    "\n",
    "    ind=ihs_preprocessing(ind)\n",
    "    \n",
    "    ###Fuel allocation\n",
    "    ind=ind.assign(fuel=np.where(((ind.FuelType1First=='Residual Fuel')|(ind.FuelType2Second=='Residual Fuel')),\n",
    "                                np.where(((ind.PropulsionType.isin(['Steam Turbine(s), Geared Drive','S.Turb, Gear & Oil Eng(s), Ele','Steam Recip(s), Direct Drive','Steam- & Gas-Turbines, Geared','Steam Turbine(s), Elec.Drive','Steam Recip(s) With Lp Turbine','Steam Turbine(s), Direct Drive','St. Turb(s) Elec Dr. Or D.E.',]))\\\n",
    "                                                                &(ind.StandardVesselType=='Liquified gas tanker')),\"LNG\",\"HFO\"),\n",
    "                                    np.where(((ind.FuelType1First=='Distillate Fuel')&(ind.FuelType2Second=='Distillate Fuel')),\"MDO\",\n",
    "                                    np.where(((ind.FuelType1First=='Distillate Fuel')&(ind.FuelType2Second.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))),\"MDO\",\n",
    "                                    np.where(((ind.FuelType1First.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))&(ind.FuelType2Second=='Distillate Fuel')),\"MDO\",\n",
    "                                    np.where(((ind.FuelType1First=='Coal')&(ind.FuelType2Second=='Distillate Fuel')),\"MDO\",\n",
    "                                    np.where(((ind.FuelType1First=='Methanol')&(ind.FuelType2Second=='Distillate Fuel')),'Methanol',\n",
    "                                        np.where((((ind.FuelType1First=='Residual Fuel')|(ind.FuelType2Second=='Residual Fuel'))&\\\n",
    "                                                ((ind.StandardVesselType=='Liquified gas tanker')&(ind.PropulsionType.isin(['Steam Turbine(s), Geared Drive','S.Turb, Gear & Oil Eng(s), Ele','Steam Recip(s), Direct Drive','Steam- & Gas-Turbines, Geared','Steam Turbine(s), Elec.Drive','Steam Recip(s) With Lp Turbine','Steam Turbine(s), Direct Drive','St. Turb(s) Elec Dr. Or D.E.',])))),'LNG',\n",
    "                                        np.where(((ind.FuelType1First=='Gas Boil Off')&(ind.FuelType2Second=='Distillate Fuel')),'LNG',\n",
    "                                        np.where(((ind.FuelType1First.isin([\"LNG\",'Lpg','Lng']))&(ind.FuelType2Second=='Distillate Fuel')),'LNG',\n",
    "                                        np.where(((ind.FuelType1First.isin([\"LNG\",'Lpg','Lng']))&(ind.FuelType2Second.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))),'LNG',\n",
    "                                        np.where(((ind.FuelType1First.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))&(ind.FuelType2Second.isin([\"LNG\",'Lpg','Lng']))),'LNG',      \n",
    "                                        np.where(ind.FuelType2Second=='Gas Boil Off','LNG',\n",
    "                                            np.where(((ind.FuelType1First=='Nuclear')&(ind.FuelType2Second=='Distillate Fuel')),'Nuclear',\n",
    "                                            np.where(((ind.FuelType1First=='Nuclear')&(ind.FuelType2Second.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))),'Nuclear',\n",
    "                                                    np.where(((ind.FuelType1First=='Coal')&(ind.FuelType2Second.isin(['Yes, But Type Not Known','Not Applicable','Unknown',None]))),'Coal',\n",
    "                                                            np.where(ind.FuelType1First=='Methanol','Methanol',                                              \n",
    "                                None))))))))))))))))\n",
    "                )\n",
    "\n",
    "\n",
    "    ###Engine types\n",
    "    ind=ind.assign(meType=np.where(ind.PropulsionType.isin(oil_eng),\n",
    "                                np.where(ind.MainEngineRPM<=300,\"SSD\",\n",
    "                                np.where(ind.MainEngineRPM.between(301,900),\"MSD\",\n",
    "                                np.where(ind.MainEngineRPM>900,\"HSD\",\"SSD\"))),\n",
    "                        np.where(ind.PropulsionType.isin(['Petrol Engine(s), Direct Drive','Petrol Engine(s), Geared Drive']),\"HSD\",       \n",
    "                        np.where(ind.PropulsionType.isin(sail),\"Sail\",\n",
    "                        np.where(ind.PropulsionType=='Battery-Electric',\"Batteries\",\n",
    "                        np.where(ind.PropulsionType=='Non-Propelled','Non-Propelled', \n",
    "                        \"SSD\"))))))\n",
    "\n",
    "    ind=ind.assign(meType=np.where(ind.fuel==\"LNG\",\n",
    "                                np.where(((ind.MainEngineModel.str.contains(\"X\"))|(ind.MainEngineModel.str.contains(\"DF\"))),\"LNG-Otto-SS\",\n",
    "                                np.where(ind.MainEngineRPM>300,\"LNG-Otto-MS\",    \n",
    "                                np.where(ind.MainEngineModel.str.contains(\"ME\"),\"LNG-Diesel\",\"LNG-Otto-MS\"                                       \n",
    "                                ))),\n",
    "                            np.where(ind.fuel==\"Methanol\",\"Methanol\", \n",
    "                                ind.meType)))\n",
    "\n",
    "\n",
    "    ##Gas turbines and Steam turbines conditional on former filters\n",
    "    ind=ind.assign(meType=np.where(((ind.PropulsionType.isin(gas_tur))|(((ind.meType.isin([\"SSD\",\"MSD\"]))&(ind.fuel==\"Gas\")))),\"Gas Turbine\",\n",
    "                        np.where(ind.PropulsionType.isin(steam),\"Steam Turbine\",\n",
    "                        ind.meType                      \n",
    "                    ))\n",
    "                )\n",
    "    ind=ind.assign(fuel=np.where(ind.meType==\"Sail\",\"Sail\",\n",
    "                        np.where(ind.meType==\"Non-Propelled\",\"Non-Propelled\",\n",
    "                        np.where(((ind.fuel.isnull())&(ind.meType==\"HSD\")),\"MDO\",\n",
    "                        np.where(((ind.fuel.isnull())&(ind.meType==\"MSD\")),\"MDO\",\n",
    "                        np.where(((ind.fuel.isnull())&(ind.meType==\"SSD\")),\"HFO\",\n",
    "                                ind.fuel)))))\n",
    "                )\n",
    "\n",
    "    ind=ind[['imo','mmsi', 'vessel_name', 'GrossTonnage', 'Deadweight', 'LengthOverallLOA',\n",
    "    'DateOfBuild', 'TEU', 'Powerkwmax', 'MainEngineModel', 'Speed', 'Speedmax', 'Speedservice', 'BreadthExtreme', 'SummerDraught', 'FuelType1Capacity',\n",
    "    'FuelType2Capacity', 'LightDisplacementTonnage', 'MainEngineRPM', 'MainEngineType', 'Powerkwservice', 'PropulsionType',\n",
    "    'TotalBunkerCapacity', 'StandardVesselType', 'imobin', 'fuel', 'meType','ais_beam','ais_loa']]\n",
    "\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e2ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/tmp/ipykernel_51/2673893709.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
      "/tmp/ipykernel_51/2673893709.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
      "/tmp/ipykernel_51/2673893709.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
      "/tmp/ipykernel_51/2673893709.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
      "/tmp/ipykernel_51/2673893709.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df[c]=df.groupby([\"StandardVesselType\", \"imobin\"])[c].apply(lambda x:x.fillna(x.mean()))\n",
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "specs = spark.createDataFrame(adapted_specs_imo(match_vessel.toPandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77bea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------\n",
      " Powerkwmax    | 0   \n",
      " Speed         | 0   \n",
      " Speedmax      | 0   \n",
      " SummerDraught | 0   \n",
      " MainEngineRPM | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Powerkwmax', 'Speed', 'Speedmax', 'SummerDraught', 'MainEngineRPM']\n",
    "\n",
    "specs.select([F.count(F.when((F.col(c)==0)|(F.col(c).isNull()), c)).alias(c) for c in columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48424b4d",
   "metadata": {},
   "source": [
    "# SAVING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52b912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"s3a://ungp-ais-data-historical-backup/user_temp/\"\n",
    "save_path_unique = save_path + \"222011485/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45cd1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as parquet\n",
    "df.write.option(\"header\",True).mode(\"overwrite\").parquet(save_path_unique + \"ais-ihs-indonesia-2022.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e60aa",
   "metadata": {},
   "source": [
    "# STOP SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a8937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ebf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Config template pyspark3.3 ais2.8 (prev ais-tt-dev)",
   "language": "python3",
   "name": "ais-tt-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
